{"pages":[{"text":"I am a Physics PhD candidate at UC Davis doing observational cosmology . Contrary to popular belief that I have to climb up to big telescopes to get data, I actually code to collect data about my astronomical targets, analyze the data to test our understanding of 'cosmological' stuff like dark matter / cosmological parameters. (Image credit: Jee et al. 2014, NASA, ESA) \"El Gordo\" - the heavy galaxy cluster that I studied. We have used the false color in blue to represent the distribution of dark matter, and red to represent the intercluster gas. My blog here is an effort for me to communicate and discuss my work flow with people from within and outside of (astro)physics. I like putting together and using reproducible research tools as I believe they can make the scientific process more transparent. Here I will put up non-technical and technical blog posts about cosmology / astrophysics / data analysis / computing, with a theme about solving problems with different research tools / programming languages. In each month, I must declare what topic I want to write about, then spend the month coming up with a blog post.","tags":"pages","loc":"http://karenyyng.github.io/pages/about.html","title":"About"},{"text":"I am working towards my PhD under the supervision of Prof. David Wittman at UC Davis. A main part of my thesis is for understanding the manifestation of dark matter properties in the extreme environment of mergers of galaxy clusters. Mergers of galaxy clusters are some of the most energetic events in the universe that are observable in a large range of wavelengths (of light). Combining data with multiple telescopes, we provide comprehensive analyses of how the merger happened even though we only have snapshots of the merger process. Check out my work with other great astrophysicists in the Merging Cluster Collaboration by visiting our website . PhD project 1 understanding the dynamics of a merging galaxy cluster We model the bimodal merger of the galaxy cluster with a Monte Carlo simulation. By making use of the shockwave observation in the radio wavelengths, we show that two main dark matter components of this particular galaxy cluster are likely to be moving towards one another for a second merger. A draft of my paper can be found on ArXiV PhD project 2 (ongoing work) characterizing galaxy-dark matter offset in galaxy clusters / groups in the Illustris simulation We aim to answer some urgent questions about the ability of merging clusters for constraining the self-interaction properties of dark matter. Illustris simulation is one of the first cosmological simulations (think: a big box of the universe modeled by a computer) with realistic galaxy / star formation. To get a sense of the relative scale of the galaxy clusters that I study, you can watch the first 15 mins of my public talk . Or you can look at the pretty pictures of the Illustris simulation . Other projects that resulted in publications 1. MC&#94;2: Galaxy Imaging and Redshift Analysis of the Merging Cluster CIZA J2242.8+5301 William A. Dawson, M. James Jee, Andra Stroe, Y. Karen Ng , Nathan Golovich, David Wittman, David Sobral M. Bruggen, H. J. A. Rottgering, R. J. van Weeren ArXiv preprint 2. Weighing \"El Gordo\" With A Precision Scale: Hubble Space Telescope Weak-Lensing Analysis of the Merging Galaxy Cluster ACT-CL J0102-4915 At z=0.87 M. James Jee, John P. Hughes, Felipe Menanteau, Cristobal Sifon, Rachel Mandelbaum, L. Felipe Barrientos, Leopoldo Infante, and Karen Y. Ng , 2014, ApJ, 785, 20, 1309.5097, doi:10.1088/0004-637X/785/1/20","tags":"pages","loc":"http://karenyyng.github.io/pages/projects.html","title":"Projects"},{"text":"The return of the merging galaxy subclusters of El Gordo? Mar 2015 This is a more updated talk on my paper at the SnowCluster conference dedicated for the study of galaxy clusters. The PDF link can be found on the SnowCluster website. Oct 2014 This is a technical talk on my paper given to my collaborator Prof. Lars Hernquist and the galaxy cluster group at the Harvard-Smithsonian Center For Astrophysics (CfA). San Francisco Amateur Astronomers - invited talk 08/20/2014 I was invited to give a public talk about my work on the merging galaxy cluster El Gordo after the press release of the mass measurement work was released. I gave a general background introduction of a galaxy cluster and talked about the anatomy of a galaxy cluster. Link to abstract Slides with video recording","tags":"pages","loc":"http://karenyyng.github.io/pages/talks.html","title":"Talks"},{"text":"Observation trip at the Keck observatory in Hawaii Dec 2013 Me on the top of Mauna Kea after my observation run at the Keck (remote) observatory control room. You can see that there are several telescope domes behind me, with the leftmost one being the Subaru telescope, and the rest are the Keck 1 and Keck 2 telescopes. I know people love to think that I (and other astronomers) had to climb up to a control room and use a joystick to control the telescope ... The truth is ... I just had to code to plan my observation and fill out forms to let the telescope operator help me... most of the observation was automated or else I might destroy the telescope ;) The \"masks\" (the metal plates with holes) that I designed for only allowing target light to reach our detector on the Keck 2 telescope. Lick Observatory workshop Oct 2012","tags":"pages","loc":"http://karenyyng.github.io/pages/trips.html","title":"Trips"},{"text":"I have been making incremental improvements to the blog, including: slight stylistic CSS change (finally know which CSS to fix) fixing jQuery plugin for returning search results properly When there is enough material I will blog about how to customize different parts of this statically generated blog using Pelican . Learning schedule Out of all the things that I wish to learn in grad. school (besides Physics and Cosmology obviously), Bayesian stat. and PGMs are some things that I have wanted to learn about for a long time. I believe a better understanding of probabilistic models / stat. concepts will aid my data analysis tremendously. Therefore, I have stopped giving myself excuses and started working on the Coursera material on Probabilistic Graphical Model (PGM) . i Related reading that I assigned myself include: which is the course textbook. which is one of my favorite textbooks with really concrete examples and accompanying code BRMLToolKit which is written in Matlab . As programming exercises, I have been trying to rewrite some of the code in Python here . I am curious to see how good the Python package libpgm is which is used in this book. Now I don't know how good the statistics / probability part of the book is but I am willing to give the python package a try without reinventing everything from scratch if it is good. Finally, since most of the books above contain mostly machine learning / probability concepts, I am adding one last book to the list to gain more perspective from the statistics side of things. It is written by one of my favorite (Bayesian) statisticians Andrew Gelman. There are other things that I wish to do later in grad. school such as actually practise doing data analysis with Kaggle competitions but I should focus! I am giving myself half a year's time from now just to learn as much about PGM / Bayesian stat. related stuff as possible before diving into Kaggle.","tags":"articles","loc":"http://karenyyng.github.io/2015-mid-year-status-and-goals.html","title":"2015 mid-year status and goals"},{"text":"Why virtualenv It's a tragedy whenever a code breaks after a software \"update\". I have recently started using virtualenv for python to prevent my macport / apt-get updates from messing up the code of my research projects. Additionally, by providing a list of python package version, it can enhance reproducibility. Before I forget how to set it up correctly, I should write about it. Prerequisites I will use python 2.7 and assume the use of macport for this post. You should also have py27-virtualenv pip installed, either with pip or macport or apt-get or if you like, from source. I think you also can replace pip with easy_install if you prefer easy_install instead. I also assume you know some basic shell commands. How to create virtualenv If you just want to use virtualenv for the same version python as the system wide version, at a terminal, call the executable of virtualenv: $ virtualenv-27 --no-site-packages ENVNAME The command could also be virtualenv for some people. I assumed that virtualenv was installed using macport creating virtualenv this way will instruct that the paths inside the virtual env not to include path the system wide python packages (using –no-site-packages seems to be the default behavior for virtualenv but let me just mention that) After this command, whatever python packages installed to the PATH variable in the shell configuration file (e.g. .bashrc / .bash_profile) will be ignored when the virtualenv is activated. Caveats Note if you have python packages path set as part of PYTHONPATH , your virtualenv will not work properly so you will want to remove any system wide python packages from your PYTHONPATH . Once the command above is done executing, a folder with ENVNAME will be created that has the structure for installing its own set of python packages. $ ls ENVNAME will show you the following directory structure bin include lib share Next, you want to figure out what packages to install and what not to install. These packages will be installed within the ENVNAME directory structure, more specifically inside: ${ APPROPRAITE_PATH } / ${ ENVNAME } /lib/python2.7/site-packages Now let's try to activate the virtual env: $ source APPROPRIATE_PATH/ENVNAME/bin/activate Then you terminal prompt will change to (ENVNAME)$ check if the pip inside this environment is a local version: (ENVNAME)$ which pip should show (ENVNAME)$ ENVNAME/bin/pip Now you can feel free to use this local version of pip to install whatever packages you want. Saving package version and reinstalling packages Let's say you have installed a bunch of packages for a specific coding project and you want to save the package used and the respective version. We can have the pip list all the python packages you have installed and save it somewhere ( ENVNAME ) $ pip freeze > packages.txt Let's say you want to set up the same virtual environment on another machine. Now you can copy over \"packages.txt\", set up the machine using the steps above, then reinstall your packages within this local environment: ( ENVNAME ) $ pip install -r packages.txt which may or may not work well depending on if you have external source of packages. Let's say you are like me and have installed some python packages from non-conventional sources, one hackish way to make sure those will be installed properly in the virtualenv is to just copy the directory containing installed python packages to the local environment: ( ENVNAME ) $ sudo cp -r PATH1 APPROPRAITE_PATH/ENVNAME/python2.7/site-packages where PATH1 is $ echo PATH1 \"/TIME_MACHINE_BACKUP/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python/2.7/site-packages/\" Now you can check if the library versions are alright: ( ENVNAME ) $ ipython In [ 1 ]: import astropy In [ 2 ]: print astropy . __version__ Out [ 2 ]: '0.4' while you can check that the system wide version of the library has a different version after deactivating the virtual env $ APPROPRAITE_PATH / $ { ENVNAME } / bin / deactivate $ ipython In [ 1 ]: import astropy In [ 2 ]: print astropy . __version__ Out [ 2 ]: '0.4.1' Automating virtualenv activation Last bit of this post is about being lazy and having the virtualenv automatically activate itself when you switch to the appropriate directory using command line. You will need the python package autoenv which can be installed by: $ pip install autoenv $ echo \"source $PATH_TO_AUTOENV /activate.sh\" >> ~/.bash_profile $ cd $APPROPRIATE_PATH / # this is the parent directory where ENVNAME lives $ echo \"source ABSOLUTE_PATH/ENVNAME/bin/activate\" > .env now whenever you switch to the appropriate directory, the virtual env will be activated for you. Troubleshooting If you see any weird behavior for importing modules in the virtualenv or when using pytest, make sure that you are using versions of python packages installed from within the virtualenv and not the system version. creating virtualenv for a different python version let's say that I use python 2.7 for most of my projects and would like to play with python 3.4 i can first install python version 3.4 using macport by: $ sudo port install python34 py34-pip then the only difference when creating the virtualenv is to specify using python 3.4 $ virtualenv -p $( which python3.4 ) ENVNAME --distribute Other virtualenv tutorials There is a wonderful tutorial from astropy.","tags":"Python","loc":"http://karenyyng.github.io/using-virtualenv-for-safeguarding-research-project-dependencies.html","title":"Using Virtualenv for safeguarding research project dependencies"},{"text":"One thing that I greatly admire my advisor, Prof. D. Wittman, for is that he is extremely well spoken and explains things very clearly. Now as a science student I think this is a very important ability that I still need to improve on. By setting up this blog I hope to force myself to explain and share with others the mini-projects / ideas that I am working / have worked on.","tags":"Personal goals","loc":"http://karenyyng.github.io/new-year-resolution-1st-post.html","title":"New year resolution + 1st post"},{"text":"Using PyTables to store / organize data is just awesome! Reading this lowers my blood pressure level. http://pytables.github.io/usersguide/tutorials.html#dealing-with-nested-structures-in-tables","tags":"Uncategorized","loc":"http://karenyyng.github.io/pytables.html","title":"PyTables!"},{"text":"It has been a wonderful week in Seattle. The organizers have really done a great job in trying to get the participants together to work on stuff. And special thanks to Phil who's done a marvelous job to talk to the participants and reboot the atmosphere when people are stuck. The progress of my little project can be viewed at: https://hackpad.com/Representing-overdensities-in-astro-data-clustering-KDE-rz6RcKo666V And there will be reports of attempts of more smaller \"hacks\" coming.","tags":"Uncategorized","loc":"http://karenyyng.github.io/astrodata-hack-week-conclusion.html","title":"AstroData Hack Week conclusion"},{"text":"Fernando Perez is giving the lecture about ipython notebook this morning. These are some notes of the interesting ideas: import all the dependencies when you try to export some cell content into a function you can use %celltofunc but it is still work in progress jinja template files for conversion to ipynb ipython nbconvert --to python --template=simplypython.tpl convert ipynb as documentation in sphinx Jake Vanderplas took over just now to teach efficient programming in numpy (!!!) he talked about how to use the reduce ufunc ... that I almost never use but is useful to know Back to Fernando about cool coding + collaboration stuff: https://gitter.im/ - for any public git repos you can a start a conversation with any git users seaborn is a nice high-level library for plotting data sets, even if they are in pandas data frame format! virtual machines with packages shipped with an ipynb that documents the data analysis of a paper, using starclusters ...... Jupyter - people are trying to make ipython notebook more inclusive since it can communicate not only with python but with Julia and R! http://jupyter.org/ Some statistics from astrophysics perspective Dan Foreman-Mackey gave a presentation on the application of Gaussian processes of detecting exolents or what's known as Kriging in geophysics. And he recommended some online books at http://www.gaussianprocess.org/gpml/","tags":"hackweek","loc":"http://karenyyng.github.io/astrodata-hack-week-day-1.html","title":"AstroData Hack Week - day 1"},{"text":"I just gave a public talk on El Gordo, the galaxy cluster that I've been working on. This is almost exactly one year after I gave a more technical talk on the same cluster in my PhD candidacy exam. And the slides are at ...... http://slideslive.com/38891780/el-gordo","tags":"Uncategorized","loc":"http://karenyyng.github.io/sfaa-talk-on-el-gordo.html","title":"SFAA talk on El Gordo"},{"text":"This is my new favorite book about Bayesian stat that I learned about from the Penn. State Astrostat summer school. Note that it's written by an astronomer and provides Mathematica code (can't get more astro/ physics than that!!! i think only physicists / astrophysicists have such a love for Mathematica which is very expensive in my opinion ... ) Today I have been reading about sample comparison: Given two samples, determine if they come from the same distribution. For such a problem, frequentists assume a null hypothesis that the samples are different then do some tests to reject the null hypothesis. Bayesians evaluate the probability of different cases when.... means being the same, variances being the same means being the same, variances being different means being different, variances being the same mean being different and variances being different For details, read Appendix C of the book. Is this what I want for answering the scientific question that I have in mind? I am not sure. I have to think harder / talk to some Bayesians.","tags":"Uncategorized","loc":"http://karenyyng.github.io/bayesian-logical-data-analysis-in-the-physical-sciences.html","title":"Bayesian Logical Data Analysis in the Physical Sciences"},{"text":"Fig Credit: (one of my favorite comics - XKCD) Today i'm reading the intro part of the book that talks about different types of telescopes. I might have read it before but i guess i should be familiar with them. When I talk with my fellow astronomer friends, I want to know which telescope they are referring to ... where it is, what it is for etc. Today I only aim to read about radio telescopes because they came first in the book. Radio wavelengths are some of the few wavelengths that we can observe on earth from lambda ~ 5 mm to ~30 m. Important features that people try to detect in the radio wavelengths include the 21-cm emission line which is a sign of neutral hydrogen 1.2 mm feature for high redshift galaxy ... maybe they r looking at X-band dropouts? (where X is some photometry bands) Important telescopes include: VLA in New Mexico Arecibo telescope in Puerto Rico which is the largest single dish radio non-steerable telescope James Clerk Maxwell telescope on Mauna Kea VLBI, very large baseline inferometry (VLBI) which consists of multiple telescopes on several continents so they can have different baselines And i know there should a few more radio arrays coming online all around the world doing exciting early universe cosmology (e.g. for the epoch of reionization). They are going to have massive data sets according to people I met at Penn. State. Hopefully they will also figure out what to do with the massive data and get good science out of them :)","tags":"Uncategorized","loc":"http://karenyyng.github.io/extragalactic-astronomy-and-cosmology-2.html","title":"Extragalactic astronomy and cosmology 2"},{"text":"I have always liked Feynman as a physicist - I felt like his lectures might be too good not to be read during grad school. Today I am reading the E&M part - since I am a E&M TA this quarter. I am starting from the very first chapter where he describes the basics - you can usually tell if the book 's good if the author can go through the basic math in a crystal clear way. I cannot believe how he verbalized Gauss' law and the concept of and divergence It is just mind blowing - every undergrad should have learned it this way - i.e. to understand what's going on before looking at the math symbols! He put: (Electric) flux = (average normal component ) * (surface area) (Magnetic) circulation = (avg. tangential component) * (distance of the loop) so that Gauss' law can simply be verbalized as flux of E field through any closed surface = enclosed charge / epsilon_naught Those are just some really very simple and easy to understand words that hides all the complicated math from first time learners. Sometimes I really hope that I have the clarity to promote physics concepts like these to my students. As a TA and a physics student myself, I experience all too often that once a student starts fearing a subject, i.e. once s/he sees the complicated math, s/he stops thinking about what's going on and just tries to pull whatever seems relevant for solving a problem. They would not make the equations / math / the physical model adapt to the problem but just substitute whatever seems convenient. Setting up the problem correctly allows you to get through 90% of the problem, the rest is usually just algebra / computation. Students often do not see that..... Anyway, it's good that the Feynman lectures are at an undergrad level so it's still quite light of a reading. Hopefully I will get through all three volumns then start reading some graduate level books such as Landau and Liftshitz.","tags":"Uncategorized","loc":"http://karenyyng.github.io/feynman-lecture-on-physics.html","title":"Feynman lecture on Physics"},{"text":"To best honest I never read the book very carefully when it was used as the text for my 2nd year cosmology data analysis class (sorry Chris). It is actually quite a fun read since it started by describing the important stat. developments throughout history in the first chapter and then goes through the basics. There isn't a lot of math so it's easy to skim through. The best point is that the examples are very relevant and point out common pitfalls. For example in astronomy a lot of stuff is in power law form, turns out a power-law distribution does not fit the formal definition of a p.d.f., (i.e. p.d.f. has to be > 0, and integrates to 1) and that the binning of data that follows power law distribution has to be taken with care etc. I would give it bonus points for including a lot of Bayesian stuff and giving good comparisons between the Frequentists and Bayesian views. :D","tags":"Uncategorized","loc":"http://karenyyng.github.io/practical-statistics-for-astronomer.html","title":"Practical Statistics for Astronomer"},{"text":"Definitely one of my favorite text on extragalactic astronomy, so fond of it that I actually owned a physical copy of the text. Today I am reading a subsection on stuff that I work on - X-ray radiation from clusters of galaxies :D","tags":"Uncategorized","loc":"http://karenyyng.github.io/extragalactic-astronomy-and-cosmology.html","title":"Extragalactic Astronomy and Cosmology"},{"text":"It is a saturday so I get to choose what type of books that I read at my \"leisure time\". As a continuation of my quest for trying to use python (well) for my research I have been investigating the possibility of using HDF5 to store some simulation results ... HDF5 is a flexible and fast file format that the theoretical astrophysicists prefer nowadays (also have to mention that the file format was first designed by my beloved college UIUC!) I like it especially because of : fast read time - library written in C, with nice Python wrapper library h5py flexible - you can choose a chunk of the data to read, not the entire dataset allows metadata to be stored - this is important coz I don't want to have to remember what exact parameters I used for creating a dataset 3 months after creating them, I want the data to be self-documented data structure organized like directory - beyond the scope of discussion here since I haven't got to that part of the book yet All in all the book was alright but they should really include a list of reference of the most commonly used commands. Here they go (for my later reference): >>> import h5py >>> import numpy as np >>> f = h5py.File(\"filepath\", \"w\") # let's just open a file to write # create two datasets with names \"array1\" and \"array2\" >>> f[\"array1\"] = np.ones((100, 1000)) # initialize a big 2D array >>> f[\"array2\"] = np.zeros((int(1e5), int(1e6))) # easy writing of metadata as attributes >>> f[\"array1\"].attrs[\"info\"] = \"big array\" >>> f[\"array2\"].attrs[\"info\"] = \"bigger array\" >>> f.close() Now this part about reading and examining the data is a bit lacking from the book, just tell me that ONE command that I need!: >>> f = h5py.File(\"filepath\", \"r\") # read only # this should give you the keys e.g. [\"array1\", \"array2\"] # so you don't have to remember what \"datasets\" are actually in the file # this is the one command to rule them all >>> f.keys() >>> f[\"array1\"].attrs.keys() # gives you the keys to call the attributes Oh and I am not sure if I didn't read the book carefully enough or if they really have a weird syntax for retrieving the actual data: # read the data back into python as a numpy array >>> arr = f[\"array1\"][...] Or if you have a linux / mac, you just want to check the file structure quickly, at the command line you can do: $ h5ls -vlr file.h5 It would spit out the descriptions of the dataset and the keys for calling the dataset. Currently not trying to do anything fancy with the hdf5 files yet but I think it 'd be good to use hdf5 in the long run. Sorry about just putting the code up like that - I meant to migrate this blog to something like Pelican ... oh well maybe another time.","tags":"Uncategorized","loc":"http://karenyyng.github.io/python-and-hdf5.html","title":"Python and HDF5"},{"text":"Was reviewing some basics in astronomy. Stuff such as the color-magnitude diagram for showing the relationship between the color and the brightness of stars (weird that astronomers call it color-magnitude coz it is really magnitude vs color diagram). Was also glad to see discussions of collisionless encounters (e.g. stuff that i work on) Great book for reviewing astrophysics overall, very clear and in depth. Will aim to read the potential theory part that has more to do with the dynamical DM simulations that I do.","tags":"Uncategorized","loc":"http://karenyyng.github.io/galactic-dynamics.html","title":"Galactic dynamics"},{"text":"Tried a couple of ways to debug python code, including using the %pdb magic within IPython or use %debug in the postmortem mode. Still couldn't figure out how to restart the code within the pdb invoked by the pdb magic without pdb raising a \"restart\" error. So I think maybe the best way is to stick with the ipython debugger, ipdb which has autocomplete and syntax highlighting: $ python -m ipdb myscript.py and insert breakpoints with: $ b mymodule.py:lineNum and debug until the next break point or restart by typing the shortcut for continue: $ c which behaves more similarly to gdb that I 'm more used to. if you want to debug a certain function from another module, set a break point to step inside that function by: $ from module.py import function $ b function and list all the arguments for this particular function: $ args We can also execute python code by: $ !my_python_code_or_command Stepping through the code is also the same as gdb $ step # or s One of my favorite commands is actually $ up which moves the debugger state to one state up the stack trace. i.e. previous line executed These are all the functionalities of pdb that I make use of so far. Other useful tips can be found at http://docs.python.org/2/library/pdb.html Trying to say goodbye to debugging with print statements.","tags":"Uncategorized","loc":"http://karenyyng.github.io/debugging-python-code-with-ipdb.html","title":"Debugging python code with ipdb"},{"text":"If you know me, then you would know that I am not a big fan of using the mouse. Back in the old days when I used linux, I could customize most of the keyboard shortcuts so it was easy to stay with the keyboard without using the mouse. Lol even though I did set up expośe with my linux box, it was just for the fun, not out of necessity. After switching to mac, I got distracted and never actually learned how to navigate with the keyboard only. It was also one of the grudges that I first had against using a Mac. But today, I have finally found what I want! Cmd Tab - helps you switch between applications Cmd ` (this is the grave accent sign, not tilde) helps you switch between different windows of the same application","tags":"Uncategorized","loc":"http://karenyyng.github.io/how-to-avoid-using-the-mouse-trackpad.html","title":"How to switch apps/windows w/o using the mouse on a Mac"},{"text":"PyData 2013 It was a really rewarding experience going to PyData and I am pleasantly surprised that the organizers have uploaded the talks already : https://vimeo.com/pydata/videos/page:1/sort:date Particularly good talks include: https://vimeo.com/63256380 - given by the CEO of Continuum Analytics talking about the next generation Python scientific computing package called Blaze which is optimized for big data analysis. https://vimeo.com/63250251 - keynote talk by Fernando Perez on ipython. more on that later.","tags":"Uncategorized","loc":"http://karenyyng.github.io/pydata-2013.html","title":"PyData 2013"}]}